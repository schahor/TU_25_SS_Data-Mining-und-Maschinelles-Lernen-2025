{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lM6IuWCJNNPc"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schahor/TU_25_SS_Data-Mining-und-Maschinelles-Lernen-2025/blob/main/02_Vorlage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEHFUTHoheSd"
      },
      "outputs": [],
      "source": [
        "# Importieren der benötigten Libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split as sklearn_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.model_selection import (\n",
        "    LeaveOneOut,\n",
        "    LeavePOut,\n",
        "    RepeatedStratifiedKFold,\n",
        "    StratifiedShuffleSplit,\n",
        "    StratifiedKFold,\n",
        "    cross_validate,\n",
        "    _split\n",
        ")\n",
        "from sklearn.datasets import make_classification, load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from sklearn.metrics import mean_squared_error as mse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Datensatz erstellen bestehend aus Datenpunkten X und Labels y\n",
        "X, y = make_classification(\n",
        "        n_samples=20,\n",
        "        n_features=2,\n",
        "        n_redundant=0,\n",
        "        class_sep=1.0,\n",
        "        n_clusters_per_class=1,\n",
        "        random_state=5,\n",
        ")"
      ],
      "metadata": {
        "id": "272xdUgoB4cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 Datenpunkt mit jeweils zwei Features\n",
        "print('Dimension: ', X.shape)\n",
        "print('Die ersten fünf Beispiele:\\n', X[:5])"
      ],
      "metadata": {
        "id": "Fpf5fYtZkGJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hilfsmethoden"
      ],
      "metadata": {
        "id": "lM6IuWCJNNPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Visualisierungsmethode\n",
        "def plot_split_dataset(\n",
        "    X_train: np.ndarray, X_test: np.ndarray, y_train: np.ndarray, y_test: np.ndarray\n",
        ") -> None:\n",
        "    plt.scatter(*X_train[y_train == 0].T, marker=\"x\", color=\"lightblue\", label=\"Class 0, Train\")\n",
        "    plt.scatter(*X_train[y_train == 1].T, marker=\"o\", color=\"lightblue\", label=\"Class 1, Train\")\n",
        "    plt.scatter(*X_test[y_test == 0].T, marker=\"x\", color=\"orange\", label=\"Class 0, Test\")\n",
        "    plt.scatter(*X_test[y_test == 1].T, marker=\"o\", color=\"orange\", label=\"Class 1, Test\")\n",
        "\n",
        "    handles = [mlines.Line2D([], [], color=\"black\", marker=marker, linestyle=\"None\", markersize=6, label=f\"Class {c}\")\n",
        "               for c, marker in zip([0, 1], [\"x\", \"o\"])]\n",
        "    handles.extend([mlines.Line2D([], [], color=color, marker=\".\", linestyle=\"None\", markersize=6, label=label)\n",
        "                    for color, label in zip([\"lightblue\", \"orange\"], [\"Train\", \"Test\"])])\n",
        "\n",
        "    plt.legend(handles=handles, loc=\"upper right\")\n",
        "\n",
        "    plt.xlabel(\"$X_1$\")\n",
        "    plt.ylabel(\"$X_2$\")\n",
        "    plt.title(\"Zwei-Klassen Dataset Split von Train- und Testsatz\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_iris_dataset(X: np.ndarray, y: np.ndarray) -> None:\n",
        "    cmap_bold = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])\n",
        "    plt.figure()\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolors=\"k\")\n",
        "    plt.title(\"Reduziertes Iris Dataset\")\n",
        "    plt.xlabel(\"Kelchblattbreite\")\n",
        "    plt.ylabel(\"Kelchblattlänge\")\n",
        "    plt.show()\n",
        "\n",
        "# Trainiert ein Classifier und berechnet die Genaugikeit\n",
        "def fit_and_score(\n",
        "    clf: BaseEstimator,\n",
        "    X_train: np.ndarray,\n",
        "    X_test: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    y_test: np.ndarray,\n",
        ") -> tuple[float, float]:\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_train_pred = clf.predict(X_train)\n",
        "    y_test_pred = clf.predict(X_test)\n",
        "\n",
        "    acc_train = accuracy_score(y_train, y_train_pred)\n",
        "    acc_test = accuracy_score(y_test, y_test_pred)\n",
        "    return acc_test, acc_train\n",
        "\n",
        "# Gibt das Trainings/Evaluierungsergebnis auf der Ausgabe aus\n",
        "def print_table(console: Console, name: str, mean_train: float,mean_test: float):\n",
        "    table = Table(title=f\"{name: <21}\")\n",
        "    table.add_column(\"Train Accuracy Mean\")\n",
        "    table.add_column(\"Test Accuracy Mean\")\n",
        "    table.add_row(f\"{mean_train * 100:3.2f} %\", f\"{mean_test * 100:3.2f} %\")\n",
        "    console.print(table)\n",
        "\n",
        "cache = {}\n",
        "\n",
        "def plot_decision_boundary(\n",
        "    model: KNeighborsClassifier, X: np.ndarray, y: np.ndarray\n",
        ") -> None:\n",
        "    h = 0.05  # step size in the mesh\n",
        "\n",
        "    # Create color maps\n",
        "    cmap_light = ListedColormap([\"#FFAAAA\", \"#AAFFAA\", \"#AAAAFF\"])\n",
        "    cmap_bold = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])\n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    k = model.n_neighbors\n",
        "    if k in cache:\n",
        "        Z = cache[k]\n",
        "    else:\n",
        "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        cache[k] = Z\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "    # Plot also the training points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n",
        "    plt.title(\"2D Iris KNN Decision Boundaries ($k=%s$)\" % k)\n",
        "\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Linear Regression Helper\n",
        "\n",
        "def plot_polyfit(x_train, y_train, x_test, y_test, y_pred, degree, test_error):\n",
        "    \"\"\"Plottet eine angepasste Polynom-Regression Kurve.\"\"\"\n",
        "    plt.title(f\"d = {degree}, test-mse = {test_error:2.3f}\")\n",
        "    plt.plot(x_test, y_pred, c=\"r\", label=\"Polynom-Regression\")\n",
        "    plt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2\\pi x)$\")\n",
        "    plt.scatter(x_train, y_train, label=\"Trainingsdaten\")\n",
        "    plt.ylim(-1.5, 1.5)\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "\n",
        "def plot_sin_curve(x_test, x_train, y_test, y_train):\n",
        "    \"\"\"Plottet eine Sinuskurve.\"\"\"\n",
        "    plt.figure()\n",
        "    plt.scatter(x_train, y_train, label=\"Trainingsdaten\")\n",
        "    plt.plot(x_test, y_test, c=\"g\", label=\"$\\sin(2 \\pi x)$\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_polynomial_fits(x_test, x_train, y_test, y_train):\n",
        "    \"\"\"Plottet Regressionskurven für verschiedene Grade.\"\"\"\n",
        "    # Plotten verschiedener Polynom-Regressionen\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for i, d in enumerate([0, 1, 5, 9]):\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "\n",
        "        # Auswertung der Polynom-Regression mit Grad d\n",
        "        error, y_pred = polynomial_regression(d, x_test, x_train, y_test, y_train)\n",
        "\n",
        "        # Plotten der Kurve\n",
        "        plot_polyfit(x_train, y_train, x_test, y_test, y_pred, d, error)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 0.85), loc=2, ncol=1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def polynomial_regression(d, x_test, x_train, y_test, y_train):\n",
        "    \"\"\"Führt eine Polynom-Regression mit Grad d durch und gibt die Test-MSE und die Test-Y-Vorhersagen zurück.\"\"\"\n",
        "    # Abbilden der Trainings- und Testdaten auf den Polynomraum mit Grad d\n",
        "    x_train_poly = map_polynomial(x_train, d)\n",
        "    x_test_poly = map_polynomial(x_test, d)\n",
        "    # Lineares Regressionsmodell im Polynomraum erstellen\n",
        "    model = LinearRegression()\n",
        "    model.fit(x_train_poly, y_train)\n",
        "    # Vorhersagen berechnen\n",
        "    y_pred = model.predict(x_test_poly)\n",
        "    # Vorhersagen evaluieren\n",
        "    test_error = mse(y_test, y_pred)\n",
        "    return test_error, y_pred"
      ],
      "metadata": {
        "id": "G5dDGh5mkwDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aufgabe 2.1 Kreuzvalidierung"
      ],
      "metadata": {
        "id": "EjG3UMGOiRHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) Train-Test-Aufteilung\n",
        "Implementieren Sie eine Methode `train_test_split` welche den Datensatz in 70% Trainingsmenge und 30% Testmenge aufteilt.\n",
        "Initialisieren Sie hierfuer einen Zufalls-[Generator](https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator) `rng`.\n",
        "Auf `rng` koennen Sie [Shuffle](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.shuffle.html) oder [Choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html) verwenden.\n",
        "\n",
        "Sofern Sie diese Aufgabe nicht lösen können, verwenden Sie [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) von sklearn."
      ],
      "metadata": {
        "id": "8eFj5-KdioKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(\n",
        "    X_data: np.ndarray, y_data: np.ndarray, test_size: float, seed: int = 2023\n",
        ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Teilt die Daten in ein Train- und Testsatz. `test_size` gibt\n",
        "    die Proportion des Testsatzes an\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Setzen der Testmenge\n",
        "test_size=0\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=test_size)\n"
      ],
      "metadata": {
        "id": "I8c4Dr2BiqRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Visualisierung\n",
        "plot_split_dataset(X_test,X_train,y_test,y_train)"
      ],
      "metadata": {
        "id": "3GUyrKfqHg-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## e) Durchführung der Kreuzvalidierung\n",
        "Implementieren Sie die Funktion `acc_kfold_cross_val`, um die gegebene Kreuzvalidierungsmethode\n",
        "durchzuführen und die mittlere Trainings- und Testgenauigkeit zu errechnen. Verwenden Sie dabei die Methode\n",
        "`sklearn.model_selection.cross_validate`"
      ],
      "metadata": {
        "id": "LChwftixJYPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def acc_kfold_cross_val(\n",
        "    clf: BaseEstimator,\n",
        "    cv: _split._BaseKFold,\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"Führt eine Kreuzvalidierung für den gegebenen Klassifizierer\n",
        "    und die Kreuzvalidierungsmethode durch und gibt die durchschnittliche Trainings- und Testgenauigkeit zurück.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n"
      ],
      "metadata": {
        "id": "KaZeZuhBJdI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Definiert die Kreuzvalidierungsmethoden\n",
        "cvs = [\n",
        "    (StratifiedKFold(n_splits=10, shuffle=True, random_state=0), \"KFold\"),\n",
        "    (\n",
        "        RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=0),\n",
        "        \"Repeated KFold\",\n",
        "    ),\n",
        "    (LeaveOneOut(), \"Leave One Out\"),\n",
        "    (LeavePOut(p=5), \"Leave P Out (p = 5)\"),\n",
        "    (\n",
        "        StratifiedShuffleSplit(n_splits=10, test_size=0.33, random_state=0),\n",
        "        \"Shuffle Split\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Output\n",
        "console = Console()\n",
        "\n",
        "# Classifiermodell\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "# Gibt die Genauigkeit fuer jede Method aus.\n",
        "# 70/30 Split\n",
        "mean_train, mean_test = fit_and_score(clf, X_train, X_test, y_train, y_test)\n",
        "print_table(console, \"70/30\", mean_train, mean_test)\n",
        "# Kreuzvalidierungsmethoden\n",
        "for cv, name in cvs:\n",
        "    mean_acc_train, mean_acc_test = acc_kfold_cross_val(clf, cv, X, y)\n",
        "    print_table(console, name, mean_acc_train, mean_acc_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "Kd8qfzorLT0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aufgabe 2.1 Kreuzvalidierung"
      ],
      "metadata": {
        "id": "pcdqfWzTR7Wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) Vorverarbeitung des Datensatzes\n",
        "In dieser Aufgabe verwenden wir einen Datensatz über Schwertlilien, bekannt als Iris-Dataset, welcher von dem\n",
        "Biologen Ronald Fisher in dem Jahre 1936 veröffentlicht wurde. Wählen Sie die ersten beiden Merkmale (engl.\n",
        "features): Kelchblattbreite (engl. sepal width) und Kelchblattlänge (engl. sepal length) über die Methode\n",
        "`load_reduced_iris_dataset` aus. Den vollständigen Iris-Datensatz können Sie über die Funktion\n",
        "[sklearn.datasets.load_iris](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) laden."
      ],
      "metadata": {
        "id": "vvKymnTZSNyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "ll-9JH7CcfpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_reduced_iris_dataset() -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Lädt den Iris-Datensatz, der auf seine ersten beiden Merkmale reduziert\n",
        "    wurde (Kelchblattbreite, Kelchblattlänge).\"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "X, y = load_reduced_iris_dataset()"
      ],
      "metadata": {
        "id": "PVmDP10NSMub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iris Visualisierung\n",
        "plot_iris_dataset(X,y)"
      ],
      "metadata": {
        "id": "9Xd0-_TvS3fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Evaluierung unterschiedlicher k-Werte\n",
        "Der einzige und Hauptparameter von kNN ist die Anzahl, wie viele nächste Nachbarn eines Datenpunktes\n",
        "entscheiden, welchem Label der Datenpunkt zugeordnet wird. Evaluieren Sie die Trainings- und Testgenauigkeit\n",
        "für alle k-Werte k ∈ [1, 3, 5, . . . ,100] mittels der Funktion `evaluate_ks`. Verwenden Sie dabei die Methode\n",
        "`evaluate` um eine Kreuzvalidierung mit 10 Splits für jeden K-Wert durchzuführen."
      ],
      "metadata": {
        "id": "bHsg3oPkUPPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(k: int, X: np.ndarray, y: np.ndarray) -> tuple[float, float]:\n",
        "    \"\"\"Führt eine 10-fache Kreuzvalidierung eines KNN-Modells\n",
        "    durch und gibt die durchschnittliche Trainingsgenauigkeit\n",
        "    und die durchschnittliche Testgenauigkeit zurück.\"\"\"\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    scores = cross_validate(\n",
        "        estimator=model, X=X, y=y, scoring=\"accuracy\", cv=10, return_train_score=True\n",
        "    )\n",
        "    return np.mean(scores[\"train_score\"]), np.mean(scores[\"test_score\"])"
      ],
      "metadata": {
        "id": "Na1S-sMkTtJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ks(\n",
        "    ks: range, X: np.ndarray, y: np.ndarray\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Bewertet jeden k-Wert des ks-Arrays und gibt deren jeweilige Trainings- und Testgenauigkeit zurück.\"\"\"\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "xBWelzfCVCpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Auswertung der Trainings und Testgenauigkeit fuer verschiedne K-Werte\n",
        "ks = range(1, 100, 1)\n",
        "acc_train, acc_test = evaluate_ks(ks, X, y)"
      ],
      "metadata": {
        "id": "7fRfUZVhVTAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Visualisierung der k-Werte\n",
        "Visualisieren Sie die k-Werte gegenüber der Traings- und Testgenauigkeit in der Funktion `plot_k_to_acc`. Der\n",
        "k-Wert wird auf die X-Achse und die Genauigkeit auf die Y-Achse abgebildet. Beschriften Sie ebenfalls beide\n",
        "Achsen."
      ],
      "metadata": {
        "id": "ESDdSvYmXS-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_k_to_acc(ks: range, acc_train: np.ndarray, acc_test: np.ndarray) -> None:\n",
        "    \"\"\"Plottet die k-Werte in Relation zu ihrer jeweiligen Trainings- und\n",
        "    Testgenauigkeit.\"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "plot_k_to_acc(ks, acc_train, acc_test)"
      ],
      "metadata": {
        "id": "W5rWgvjdV13c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d) Bester k-Wert\n",
        "Bestimmen Sie anhand der zuvor erhaltenen Zwischenergebnisse den besten k-Wert über die Funktion\n",
        "`get_best_k`."
      ],
      "metadata": {
        "id": "gCo_WNkIZX8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_k(ks: range, acc_test: np.ndarray) -> int:\n",
        "    \"\"\"Basierend auf der höchsten Testgenauigkeit gibt diese\n",
        "    Funktion den besten Wert für k zurück.\"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "get_best_k(ks, acc_test)"
      ],
      "metadata": {
        "id": "4563fhFuWBUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### e) Entscheidungsregionen\n",
        "Wir können auch die Entscheidungsregionen visualisieren, die ein kNN-Klassifikator für gegebene Werte von k\n",
        "erstellt. Erzeugen Sie einen kNN-Klassifikator und stellen Sie dessen Entscheidungsregionen visuell in der\n",
        "Methode `plot_decision_boundary_for_k` dar."
      ],
      "metadata": {
        "id": "mlCCT2rkaopV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decision_boundary_for_k(k: int, X: np.ndarray, y: np.ndarray) -> None:\n",
        "    \"\"\"Erstellt und passt ein KNN-Modell mit dem Wert k an und plottet\n",
        "    dessen Entscheidungsgrenze.\"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "for k in range(1, 101, 20):\n",
        "        plot_decision_boundary_for_k(k, X, y)"
      ],
      "metadata": {
        "id": "5eatUhcdZwO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Regression (Fortsetzung)"
      ],
      "metadata": {
        "id": "y-kXYZHMeprA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Polynomiale Regression\n",
        "Wir können die lineare Regression leicht auf nicht-lineare Eingangsvariablen-Zielvariablen-Korrelationen\n",
        "ausdehnen. Daher ist es nur notwendig, unsere Eingabe in einen Merkmalsraum abzubilden, in dem die Merkmale\n",
        "unserer Meinung nach linear mit der Zielvariablen korreliert sind. Ein gutes Beispiel ist die Polynomiale\n",
        "Regression, die den Eingabedatenpunkt x nimmt und ihn auf den Satz polynomialer Merkmale (x0, x1, x2, . . . , xd)\n",
        "abbildet, wobei d der Grad der Polynomtransformation ist.\n",
        "Implementieren Sie die Funktion `map_polynomial`, welche diese Operation durchführt."
      ],
      "metadata": {
        "id": "xdIP5BOteti-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_polynomial(x: np.ndarray, degree: int) -> np.ndarray:\n",
        "    \"\"\"Erstellt eine Polynomtransformation vom Grad d:\n",
        "    Mapt jeden Datenpunkt x_i, x_(i+1), ..., x_k auf sein Polynom:\n",
        "     [x_i, x_(i+1), ..., x_k] -> [[x_i^0,     ..., x_i^d],\n",
        "                                  [x_(i+1)^0, ..., x_(i+1)^d],\n",
        "                                              ....\n",
        "                                  [x_k^0,     ..., x_k^d]].\"\"\"\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "vccwp4-QesXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wir definieren eine Funktion, welche Float-Zahlen ins Bogenmaß\n",
        "# übersetzt und wenden diese auf den Sinus an.\n",
        "def fsin(x: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"Mappt eine Float-Zahl auf die ausgewertete Sinus-Funktion.\"\"\"\n",
        "  return np.sin(2 * np.pi * x)\n",
        "\n",
        "# Daten Generierung\n",
        "x_train = np.linspace(0, 1, 10).reshape(-1, 1)\n",
        "y_train = fsin(x_train) + np.random.normal(scale=0.25, size=x_train.shape)\n",
        "x_test = np.linspace(0, 1, 100).reshape(-1, 1)\n",
        "y_test = fsin(x_test)\n",
        "\n",
        "# Plotting\n",
        "plot_sin_curve(x_test, x_train, y_test, y_train)\n",
        "plot_polynomial_fits(x_test, x_train, y_test, y_train)"
      ],
      "metadata": {
        "id": "vxiTmzM1fScx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1ZDPjXggcFu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}