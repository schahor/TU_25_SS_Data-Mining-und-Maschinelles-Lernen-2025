{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EY-Yvt_Hmbq5",
        "m1W3b9B0AASy"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schahor/TU_25_SS_Data-Mining-und-Maschinelles-Lernen-2025/blob/main/08_Vorlage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import sklearn.kernel_approximation as ka\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from matplotlib.colors import Normalize\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_circles, make_moons\n",
        "\n",
        "\n",
        "sns.set(\n",
        "    context=\"notebook\",\n",
        "    style=\"whitegrid\",\n",
        "    rc={\"figure.dpi\": 120, \"scatter.edgecolors\": \"k\"},\n",
        ")"
      ],
      "metadata": {
        "id": "82bDXDqWWCY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Uebung 8 SVM & Clustering"
      ],
      "metadata": {
        "id": "EjG3UMGOiRHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hilfsmethoden"
      ],
      "metadata": {
        "id": "EY-Yvt_Hmbq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache = {}\n",
        "\n",
        "\n",
        "def plot_support_vectors(X, y, clf, title=None):\n",
        "    # plot the line, the points, and the nearest vectors to the plane\n",
        "    plt.figure()\n",
        "    plt.clf()\n",
        "\n",
        "    plt.scatter(\n",
        "        clf.support_vectors_[:, 0],\n",
        "        clf.support_vectors_[:, 1],\n",
        "        s=90,\n",
        "        facecolors=\"none\",\n",
        "        zorder=10,\n",
        "        edgecolors=\"k\",\n",
        "    )\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired, edgecolors=\"k\")\n",
        "\n",
        "    plt.axis(\"tight\")\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() * 0.90, X[:, 0].max() * 1.1\n",
        "    y_min, y_max = X[:, 1].min() * 0.90, X[:, 1].max() * 1.1\n",
        "\n",
        "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
        "    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(XX.shape)\n",
        "    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
        "    plt.contour(\n",
        "        XX,\n",
        "        YY,\n",
        "        Z,\n",
        "        colors=[\"k\", \"k\", \"k\"],\n",
        "        linestyles=[\"--\", \"-\", \"--\"],\n",
        "        levels=[-1, 0, 1],\n",
        "    )\n",
        "\n",
        "    plt.xlim(x_min, x_max)\n",
        "    plt.ylim(y_min, y_max)\n",
        "\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot a set of digits\n",
        "def plot_digits(data, title):\n",
        "    fig, axes = plt.subplots(\n",
        "        4,\n",
        "        10,\n",
        "        figsize=(10, 4),\n",
        "        subplot_kw={\"xticks\": [], \"yticks\": []},\n",
        "        gridspec_kw=dict(hspace=0.1, wspace=0.1),\n",
        "    )\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(\n",
        "            data[i].reshape(8, 8), cmap=\"binary\", interpolation=\"nearest\", clim=(0, 16)\n",
        "        )\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "class MidpointNormalize(Normalize):\n",
        "    \"\"\"Source: https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\"\"\"\n",
        "\n",
        "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
        "        self.midpoint = midpoint\n",
        "        Normalize.__init__(self, vmin, vmax, clip)\n",
        "\n",
        "    def __call__(self, value, clip=None):\n",
        "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
        "        return np.ma.masked_array(np.interp(value, x, y))\n",
        "\n",
        "\n",
        "def visualize_gridsearch(\n",
        "    gs, range_a, range_b, xlabel, ylabel, title, logx=False, fignum=\"121\", cbar=False\n",
        "):\n",
        "    scores = gs.cv_results_[\"mean_test_score\"].reshape(len(range_a), len(range_b))\n",
        "    plt.subplot(fignum)\n",
        "    plt.subplots_adjust(left=0.2, right=0.95, bottom=0.15, top=0.95)\n",
        "    plt.imshow(\n",
        "        scores,\n",
        "        interpolation=\"nearest\",\n",
        "        cmap=plt.cm.hot,\n",
        "        norm=MidpointNormalize(vmin=0.1, midpoint=0.95),\n",
        "    )\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    if cbar:\n",
        "        plt.colorbar()\n",
        "    plt.yticks(\n",
        "        np.arange(len(range_a)), [\"$2^{\" + str(int(x)) + \"}$\" for x in np.log2(range_a)]\n",
        "    )\n",
        "    if logx:\n",
        "        plt.xticks(\n",
        "            np.arange(len(range_b)),\n",
        "            [\"$2^{\" + str(int(x)) + \"}$\" for x in np.log2(range_b)],\n",
        "            rotation=45,\n",
        "        )\n",
        "    else:\n",
        "        plt.xticks(np.arange(len(range_b)), range_b, rotation=45)\n",
        "    plt.title(title)\n",
        "    plt.grid(False)\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    \"\"\"Create a decision boundary plot that shows the predicted label for each point.\"\"\"\n",
        "    h = 0.05  # step size in the mesh\n",
        "\n",
        "    # Create color maps\n",
        "    cmap_light = ListedColormap([\"#FFAAAA\", \"#AAFFAA\", \"#AAAAFF\"])\n",
        "    cmap_bold = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])\n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    k = model.n_neighbors\n",
        "    if k in cache:\n",
        "        Z = cache[k]\n",
        "    else:\n",
        "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        cache[k] = Z\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "    # Plot also the training points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n",
        "    plt.title(\"2D Iris KNN Decision Boundaries ($k=%s$)\" % k)\n",
        "\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.show()\n",
        "\n",
        "def plot_kmeans(X, k, title, subplot=None):\n",
        "    \"\"\"# Helper function to run K-Means with given k and plot clustering\"\"\"\n",
        "    # Run K-Means\n",
        "    labels, centers = fit_kmeans(X=X, k=k)\n",
        "\n",
        "    # Plot resulting clustering\n",
        "    plot_clustering(X, labels, centers, title, subplot)"
      ],
      "metadata": {
        "id": "dHLxibjymfsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) SVM\n"
      ],
      "metadata": {
        "id": "sa0CwItSWDhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Daten Generierung"
      ],
      "metadata": {
        "id": "m1W3b9B0AASy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data\n",
        "def plot_data(X, y):\n",
        "    \"\"\"Plots the data and label assignment as a scatter plot.\"\"\"\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors=\"k\", zorder=10)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.axis(\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "X_lin, y_lin = datasets.make_blobs(n_samples=40, centers=2, random_state=6)\n",
        "\n",
        "plot_data(X_lin, y_lin)"
      ],
      "metadata": {
        "id": "aYEtZDqo__Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a) Lineare Kernel"
      ],
      "metadata": {
        "id": "j71Vx1iYl3Ap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beginnen wir damit den gegebenen linearen separierbaren Datensatz, der aus zwei Klassen-Clustern besteht mittels SVM zu trennen.\n",
        "Verwenden Sie das Modul \\texttt{sklearn.svm.SVC} in der Funktion \\texttt{fit\\_linear\\_svm}, um die Stützvektoren zu finden, die die Hyperebene in den Daten definieren, und die Spanne zu den nächstgelegenen Datenpunkten maximiert.\n",
        "\n",
        "Verwenden Sie dabei keine Regularisierung ($C = 10^{10}$), sodass alle Stützvektoren so definiert, dass sie die Datenpunkte sind, die auf dem Rand der Spanne liegen und den exakt gleichen orthogonalen Abstand zur trennenden Hyperebene (den Randwert) haben."
      ],
      "metadata": {
        "id": "R176R6CeCudk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_linear_svm(X, y):\n",
        "    \"\"\"Fits a svm with a linear kernel and no regularization to the data.\"\"\"\n",
        "    clf = ...\n",
        "    return clf"
      ],
      "metadata": {
        "id": "H7ykmPtzr3TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an SVC object with a linear kernel (and no regularization)\n",
        "# Fit the hyperplane to the data\n",
        "clf = fit_linear_svm(X_lin, y_lin)\n",
        "\n",
        "# Plot data with hyperplane and support vectors\n",
        "plot_support_vectors(X_lin, y_lin, clf)"
      ],
      "metadata": {
        "id": "g4WZ3sHYsbkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b) Kernel-Trick\n",
        "\n",
        "SVMs erlauben die Verwendung des sogenannten _Kernel-Tricks_ für Probleme, die nicht wie oben angenommen linear trennbar sind, d.h. die Berechnung des Vektorprodukts in einem Merkmalsraum, in welchem die Daten erneut linear seperarierbar sind. Bekannte Kernel sind:\n",
        "\n",
        "- Linear Kernel\n",
        "- _d_-Grad-Polynom-Kernel\n",
        "- Radiale Basisfunktionskernel (RBF)\n",
        "\n",
        "Wir müssen daher eine Hyperebene in einem anderen Kernelraum finden, in dem die Daten wieder trennbar werden. Verwenden wir nun den radialen Basisfunktion Kernel $k_{rbf}(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp( - \\frac{||\\mathbf{x}_i - \\mathbf{x}_j|||^2}{2 \\sigma^2})$ in der Methode `fit_rbf_svm` um unseren Merkmalsraum zu transferieren.\n",
        "\n",
        "Hinweis: Sklearn hat diesen Kernel bereits implementiert."
      ],
      "metadata": {
        "id": "I080iO67r9xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Daten Generierung\n"
      ],
      "metadata": {
        "id": "3oc0HoJ3E27R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create circles dataset\n",
        "X_circ, y_circ = datasets.make_circles(\n",
        "    n_samples=50, noise=0.2, random_state=6, factor=0.01\n",
        ")\n",
        "\n",
        "# Plot\n",
        "plot_data(X_circ, y_circ)"
      ],
      "metadata": {
        "id": "2Z0h-xXQE2PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create an SVC object with the linear kernel parameter\n",
        "clf = fit_linear_svm(X_circ, y_circ)\n",
        "\n",
        "# Plot data with hyperplane and support vectors\n",
        "plot_support_vectors(X_circ, y_circ, clf)\n"
      ],
      "metadata": {
        "id": "KfxCvfYYErC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_rbf_svm(X, y):\n",
        "    \"\"\"Fits a svm with a rbf kernel and no regularization to the data.\"\"\"\n",
        "    clf = ...\n",
        "    return clf\n"
      ],
      "metadata": {
        "id": "OitoyXOzwfMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an SVC object with the rbf kernel parameter\n",
        "clf = fit_rbf_svm(X_circ, y_circ)\n",
        "\n",
        "# Plot data with hyperplane and support vectors\n",
        "plot_support_vectors(X_circ, y_circ, clf)"
      ],
      "metadata": {
        "id": "UhpbpBpPFd2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1c) Regularisierung\n",
        "\n",
        "Ein weiterer wichtiger Hyperparameter für die SVM ist die Regularisierung. Die SVM-Optimierung versucht, Datenpunkte zu finden, die die Spanne definieren. Dies ist nicht immer die beste Lösung, da die Daten verrauscht sein können und die trennende Hyperebene das Rauschen überlagert. Daher ist es üblich, einige Datenpunkte \\textbf{in} der Spanne zu erlauben, die als _Slack Variablen_ bezeichnet werden. Diese Regularisierung wird mit dem _C_ Parameter in `svm.SVC(...)` implementiert. Das sind die Kosten, die entstehen, wenn man zulässt, dass Supportvektoren innerhalb des Spannraums liegen. Die Zuweisung hoher Kosten (z.B. _C=1e10_) wird dazu führen, dass keine Datenpunkte innerhalb der Spanne liegen, während die Zuweisung niedriger Kosten (z.B. _C=1e-1_) zu einer weniger strengen Spanne führt.\n",
        "\n",
        "Visualisieren Sie die Entscheidungsregionen und Support-Vektoren für $C=[2^{-6}, 2^{-3}, 1, 2^3, 2^6, 2^{12}]$ in der Methode `plot_svm_C`.\n",
        "Sie können dabei auf die Funktion `plot_support_vectors` zurückgreifen.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-X6kGICAyZ1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_svm_C(X, y):\n",
        "    \"\"\"Plot the decision boundaries and support vectors for regularization constants:\n",
        "    C=[2^-6, 2^-3, 1, 2^3, 2^6, 2^12].\"\"\"\n",
        "    # min = -6, max = 12\n",
        "    # loop\n",
        "        # Create an SVC object with the rbf kernel parameter\n",
        "\n",
        "\n",
        "        # Fit the hyperplane to the data\n",
        "\n",
        "\n",
        "        # Plot data with hyperplane and support vectors\n"
      ],
      "metadata": {
        "id": "p59HDyvDW_lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "pjjauejXYaNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate the two moons dataset\n",
        "X_moons, y_moons = datasets.make_moons(200, noise=0.4, random_state=6)\n",
        "# Plot the data\n",
        "plot_data(X_moons, y_moons)\n",
        "plot_svm_C(X_moons, y_moons)"
      ],
      "metadata": {
        "id": "m2_lUMTiXf59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0nmLNUZkN9yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) SVM Multi-Label Klassifikation"
      ],
      "metadata": {
        "id": "3sVcjlGVOc67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Generierung"
      ],
      "metadata": {
        "id": "Kg_nbXsnPx3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load digits\n",
        "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
        "\n",
        "# Show data\n",
        "plot_digits(X_digits, title=\"Digits\")\n"
      ],
      "metadata": {
        "id": "yYToWV7vP0mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2a) SVM Gridsearch\n",
        "Wir werden den Zifferndatensatz verwenden, um die Genauigkeit der SVM in Bezug auf die verschiedenen Hyperparameter-Setups zu messen.\n",
        "\n",
        "Support-Vektor-Maschinen sind leistungsfähige Klassifikatoren. Es ist wichtig, eine Menge von Hyperparametern zu finden, die gut zu den Daten passen. Dazu ist es üblich, eine Hyperparametersuche z.B. über eine Gridsearch durchzuführen, das sich über mögliche Werte für verschiedene Parameter erstreckt. Für den RBF-Kernel können wir z.B. ein Gridsearch mit den Parametern _C_ und $\\gamma$ abdecken, was effektiv jede Kombination beider Parameterbereiche aufbaut.\n",
        "\n",
        "Führen Sie eine Gridsearch in der Methode `run_grid_search` für die Bereiche $2^{-13} \\leq \\gamma \\leq 2^{-4}$, $2^{-3} \\leq C \\leq 2^{3}$, $1 \\leq d \\leq 6$ durch. Sie können dabei die Funktion `sklearn.model_selection.GridSearchCV` verwenden."
      ],
      "metadata": {
        "id": "VUBOO6BkO-1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_grid_search(X_digits, y_digits, C_range, degree_range, gamma_range):\n",
        "    \"\"\"Defines and runs a grid search over the given hyperparameter ranges for\n",
        "    rbf and polynomial kernel. Each run is evaluated by using a 5-fold-cross-validation.\"\"\"\n",
        "    # Define grid over parameters for rbf and poly kernel\n",
        "\n",
        "    # GridSearch arguments\n",
        "\n",
        "    # Run gridsearch on the RBF grid\n",
        "\n",
        "    return poly_gs, rbf_gs\n"
      ],
      "metadata": {
        "id": "SrB_Z1HfXwif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter ranges\n",
        "gamma_range = [2 ** i for i in range(-13, -4)]\n",
        "C_range = [2 ** i for i in range(-3, 4)]\n",
        "degree_range = range(1, 7)\n",
        "\n",
        "poly_gs, rbf_gs = run_grid_search(X_digits, y_digits, C_range, degree_range, gamma_range)\n",
        "\n",
        "# Visualize the gridsearch results\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.suptitle(\"GridSearch Validation Accuracy\")\n",
        "visualize_gridsearch(\n",
        "    poly_gs,\n",
        "    C_range,\n",
        "    degree_range,\n",
        "    ylabel=\"C\",\n",
        "    xlabel=\"degree\",\n",
        "    title=\"SVM Poly Kernel\",\n",
        "    fignum=\"121\",\n",
        ")\n",
        "visualize_gridsearch(\n",
        "    rbf_gs,\n",
        "    C_range,\n",
        "    gamma_range,\n",
        "    ylabel=\"C\",\n",
        "    xlabel=\"$\\gamma$\",\n",
        "    title=\"SVM RBF Kernel\",\n",
        "    logx=True,\n",
        "    fignum=\"122\",\n",
        "    cbar=True,\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wkq2Tz-3Qgab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) K-Means Clustering"
      ],
      "metadata": {
        "id": "zIDmYN9USF2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3a) Datensatzvisualisierung\n",
        "\n",
        "Beginnen wir mit einem Beispieldatensatz, der aus drei getrennte Cluster besteht.\n",
        "Wenden Sie auf diesen K-Means Clustering an (`fit_kmeans`).\n",
        "Stellen Sie anschließend die gefundenen Cluster in der Methode `plot_clustering` dar.\n"
      ],
      "metadata": {
        "id": "yulMzGMsTYWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate three clusters\n",
        "X, y = make_blobs(\n",
        "    n_samples=1000, centers=3, cluster_std=0.6, n_features=2, random_state=0\n",
        ")\n",
        "\n",
        "def plot_cluster_data(X):\n",
        "    \"\"\"Creates a scatter plot for the given data.\"\"\"\n",
        "    plt.figure()\n",
        "    plt.scatter(X[:, 0], X[:, 1])\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Plot data\n",
        "plot_cluster_data(X)\n"
      ],
      "metadata": {
        "id": "xjwNTu10UQlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def fit_kmeans(X, k, init=\"k-means++\", n_init=10, random_state=0) -> [np.ndarray, np.ndarray]:\n",
        "    \"\"\"Runs K-Means clustering and returns the label assignments and cluster centroids.\"\"\"\n",
        "    # Build K-Means model\n",
        "\n",
        "    # Fit model\n",
        "\n",
        "    # Predict cluster assignment for each datapoint\n",
        "\n",
        "    # Get cluster centers\n",
        "\n",
        "    return labels, centers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BgNfOx3sUBI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, centers = fit_kmeans(X, k=3)\n",
        "\n",
        "print(f\"Labels: {labels}, {centers}\")"
      ],
      "metadata": {
        "id": "omYF9ZIPVy5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_clustering(X, labels, centers=None, title=\"\", subplot=None):\n",
        "    \"\"\"Helper function to plot the clustering\"\"\"\n",
        "    # Plot in given subplot\n",
        "    if subplot:\n",
        "        plt.subplot(subplot)\n",
        "\n",
        "    # Plot data with labels as color\n",
        "\n",
        "\n",
        "    # Plot centers if given\n",
        "\n",
        "\n",
        "    # Set title\n",
        "\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "Neqi3mRNVHP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot clustering\n",
        "plt.figure()\n",
        "plot_clustering(X, labels, centers)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t8VaoSVOUbPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3b) Iterationen\n",
        "\n",
        "K-Means hat erfolgreich drei separate Cluster gefunden.\n",
        "\n",
        "Nun möchten wir die Bewegung der Cluster-Zentroiden über die Funktion `plot_kmeans_iterations` nach jeder Iteration visualisieren."
      ],
      "metadata": {
        "id": "6RgOHYSpWmw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_kmeans_iterations(X, guess_center, initial_labels):\n",
        "    \"\"\"Plots the cluster assignments after iteration 1 to 6.\"\"\"\n",
        "\n",
        "    # loop\n",
        "        # Stop K-Means after i iterations\n",
        "\n",
        "\n",
        "        # Plot clustering for current iteration\n",
        "\n"
      ],
      "metadata": {
        "id": "lE3AC4DKXWNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 3 clusters\n",
        "true_centers = [[0, 0], [1, 0], [0, 1]]\n",
        "X, y = make_blobs(\n",
        "    n_samples=900, centers=true_centers, cluster_std=0.25, n_features=2, random_state=0\n",
        ")\n",
        "\n",
        "# Generate random centers\n",
        "np.random.seed(2)\n",
        "guess_center = np.array(true_centers) + np.random.randn(3, 2) * 0.5\n",
        "\n"
      ],
      "metadata": {
        "id": "jgCQCdDdWWRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each datapoint find closest center\n",
        "initial_labels = np.argmin(\n",
        "    np.sqrt(((X - guess_center.reshape(3, 1, 2)) ** 2).sum(2)), 0\n",
        ")\n",
        "\n",
        "plot_kmeans_iterations(X, guess_center, initial_labels)"
      ],
      "metadata": {
        "id": "U2AS0Vp-XLyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_tb_eMk7XxhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beispiele zur Wichtigkeit der Hyperparameter"
      ],
      "metadata": {
        "id": "R9o4S9YCYZV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate four clusters with custom centers\n",
        "X, y = make_blobs(\n",
        "    n_samples=[100, 100, 100, 500],\n",
        "    centers=[[0, 0], [0, 0.25], [0.15, 0.15], [0.5, 1]],\n",
        "    cluster_std=[0.04, 0.04, 0.04, 0.1],\n",
        "    n_features=2,\n",
        "    random_state=0,\n",
        ")\n",
        "\n",
        "# Run kmeans twice with two different random states\n",
        "labels_a, centers_a = fit_kmeans(X=X, k=4, n_init=1, init=\"random\", random_state=0)\n",
        "labels_b, centers_b = fit_kmeans(X=X, k=4, n_init=1, init=\"random\", random_state=1)\n",
        "\n",
        "# Plot cluster assignments and cluster centers\n",
        "plt.figure(figsize=(10, 5))\n",
        "plot_clustering(X, labels_a, centers_a, title=\"Bad Clustering\", subplot=121)\n",
        "plot_clustering(X, labels_b, centers_b, title=\"Good Clustering\", subplot=122)\n",
        "plt.show()\n",
        "\n",
        "# Generate three clusters\n",
        "X, y = make_blobs(\n",
        "    n_samples=1000, centers=3, cluster_std=0.6, n_features=2, random_state=0\n",
        ")\n",
        "\n",
        "# Run and plot\n",
        "plt.figure(figsize=(15, 5))\n",
        "plot_kmeans(X, k=2, title=\"$K=2$, Too Few\", subplot=131)\n",
        "plot_kmeans(X, k=3, title=\"$K=3$, Correct\", subplot=132)\n",
        "plot_kmeans(X, k=4, title=\"$K=4$, Too Many\", subplot=133)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YBh_AcerYWah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problematische Clustering Beispiele"
      ],
      "metadata": {
        "id": "BkilCMHCbKYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make data\n",
        "X_circles, _ = make_circles(n_samples=1000, factor=0.5, noise=0.05)\n",
        "X_moons, _ = make_moons(1000, noise=0.03, random_state=0)\n",
        "\n",
        "# Run K-Means and Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plot_kmeans(X_circles, k=2, title=\"Circles\", subplot=121)\n",
        "plot_kmeans(X_moons, k=2, title=\"Moons\", subplot=122)\n",
        "plt.show()\n",
        "\n",
        "# Make clusters with different variances\n",
        "X, y = make_blobs(\n",
        "    n_samples=2000, centers=[[0.5, 0.0], [0.5, 5.5]], cluster_std=[0.15, 2.0]\n",
        ")\n",
        "\n",
        "# Plot true and K-Means clustering\n",
        "plt.figure(figsize=(10, 5))\n",
        "plot_clustering(X, labels=y, title=\"True Clustering\", subplot=121)\n",
        "plot_kmeans(k=2, X=X, title=\"K-Means Clustering\", subplot=122)\n",
        "plt.show()\n",
        "\n",
        "# Make data\n",
        "X = np.random.rand(5000, 2)\n",
        "\n",
        "# Run and plot K-Means\n",
        "plt.figure(figsize=(10, 5))\n",
        "plot_clustering(X, labels=np.zeros(X.shape[0]), title=\"Uniform Data\", subplot=121)\n",
        "plot_kmeans(k=10, X=X, title=\"K-Means Clustering\", subplot=122)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WQibM6-lYoPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "585FHlTTaJ8D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}